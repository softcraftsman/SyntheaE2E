{
	"name": "2 - Problems with data access",
	"properties": {
		"folder": {
			"name": "NOAA"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "smallSpark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "81f7487d-460e-43d9-9b24-d351e0510d21"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/de4c6b08-3d24-452b-80f7-97b7ce4e37d1/resourceGroups/Healthcare/providers/Microsoft.Synapse/workspaces/fauxuni/bigDataPools/smallSpark",
				"name": "smallSpark",
				"type": "Spark",
				"endpoint": "https://fauxuni.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/smallSpark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Problems with data access\n",
					"\n",
					"This notebook performs a similar operation as the last notebook: loading some NWM data from Azure Blob Storage. But instead of visualizing the data, we'll focus into some potential performance issues with *accessing* the data as-is. We'll compare two ways of getting the data:\n",
					"\n",
					"1. A \"download\" workflow, where you download the files ahead of time to your local disk\n",
					"2. A \"cloud-native\" workflow, where you read the data directly from Blob Storage.\n",
					"\n",
					"\n",
					"## Background\n",
					"\n",
					"First, some background on different \"styles\" of working that the Pangeo community have identified.\n",
					"\n",
					"\n",
					"![](download-model.png)\n",
					"\n",
					"Under the \"download\" style of working, there are two distinct phases. An initial download phase, were data are downloaded from the HTTP / FTP / whatever server to your local workspace (laptop, workstation, HPC, etc.). Once the data is downloaded, you start your iterative clean / transform / analyze / visualize cycle. This model works alright for smaller, static datasets. It breaks down for datasets that are updating frequently, or are so large that downloading the archive isn't an option.\n",
					"\n",
					"![](cloud-native-model.png)\n",
					"\n",
					"Under the \"cloud-native\" model, there isn't an initial download phase; the data stay where they are in Azure Blob Storage. Instead, you read the data directly into memory on your compute. Crucially, the compute is deployed in the same Azure region as the data, which gives you a nice, high-bandwidth connection between the storage and compute services.\n",
					"\n",
					"Unsurprisingly, we're big fans of the cloud-native model. While there are some nuances, it's a lower barrier to entry for newcomers. And it scales extremely well to large datasets (depending on the format and access pattern, as we'll dig into now)."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#%pip install pooch"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": []
				},
				"source": [
					"import adlfs\n",
					"import azure.storage.blob\n",
					"import planetary_computer\n",
					"import xarray as xr\n",
					"\n",
					"fs = adlfs.AzureBlobFileSystem(\n",
					"    \"noaanwm\", credential=planetary_computer.sas.get_token(\"noaanwm\", \"nwm\").token\n",
					")\n",
					"# force xarray to import everything\n",
					"xr.tutorial.open_dataset(\"air_temperature\");"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Download Workflow\n",
					"\n",
					"First, we'll use the classic \"download model\" workflow style, where we download the data to disk ahead of time."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": []
				},
				"source": [
					"import urllib.request"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": []
				},
				"source": [
					"print(\"Downloading from Blob Storage\")\n",
					"%time filename, response = urllib.request.urlretrieve(\"https://noaanwm.blob.core.windows.net/nwm/nwm.20230123/short_range/nwm.t00z.short_range.land.f001.conus.nc\")\n",
					"print(\"-\" * 80)\n",
					"\n",
					"print(\"Reading metadata\")\n",
					"%time ds = xr.open_dataset(filename)\n",
					"print(\"-\" * 80)\n",
					"\n",
					"print(\"Loading data\")\n",
					"%time ds = ds[\"SOILSAT_TOP\"].load()"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Timing will vary a bit, but we're seeing *roughly* 500 ms to download the data, 20 ms to read the metadata, and 500 ms to load the data from disk."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Cloud-native model\n",
					"\n",
					"The `open_dataset` in the last notebook, reading from blob storage, might have felt a bit slow. Let's do some timings and logging to see what's going on."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": []
				},
				"source": [
					"import logging\n",
					"import pathlib\n",
					"import azure.core.pipeline.policies\n",
					"\n",
					"p = pathlib.Path(\"log.txt\")\n",
					"p.unlink(missing_ok=True)\n",
					"\n",
					"# Ensure range requests are logged\n",
					"azure.core.pipeline.policies.HttpLoggingPolicy.DEFAULT_HEADERS_WHITELIST.add(\n",
					"    \"Content-Range\"\n",
					")\n",
					"\n",
					"logger = logging.getLogger()\n",
					"logging.basicConfig(level=logging.DEBUG, filename=\"log.txt\")"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": []
				},
				"source": [
					"%%time\n",
					"prefix = \"nwm/nwm.20230123\"\n",
					"\n",
					"ds = xr.open_dataset(\n",
					"    fs.open(f\"{prefix}/short_range/nwm.t00z.short_range.land.f001.conus.nc\")\n",
					")\n",
					"ds"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"So about 1 – 1.5 seconds *just to read the metadata*. Let's load up some data too."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": []
				},
				"source": [
					"logger.info(f\"{' Reading Data ':=^80}\")\n",
					"\n",
					"%time soil_saturation = ds[\"SOILSAT_TOP\"].load()"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"And another 1 – 1.5 seconds to read the data. The logs will help us figure out what's going on.\n",
					"\n",
					"## Inspecting the logs\n",
					"\n",
					"We wrote a bunch of output to `log.txt`, which we'll go through now. For some context, the overall workflow here is `xarary` uses `h5netcdf` to load the NetCDF file. `h5netcdf` will \"open\" the \"file\" we give it, and do a bunch of seeks and reads to read the HDF5 file format. But, crucially, we don't have a regular file here. Instead, we have this `fsspec.OpenFile` thing. When `h5netcdf` reads the first eight bytes, fsspec will go off and make an HTTP request to download that data from Blob Storage.\n",
					"\n",
					"So to understand the performance, we'll want to look for file reads and HTTP requests.\n",
					"\n",
					"\n",
					"1. Look at the number of \"reads\" by xarray / h5netcdf (~ 130!)\n",
					"2. Count the number of HTTP requests (~ 13!)\n",
					"\n",
					"To summarize the timings\n",
					"\n",
					"| Stage | Download | Stream |\n",
					"| --- | --- | --- |\n",
					"| Download | 0.5 | - |\n",
					"| Metadata | 0.02 | 1.5 |\n",
					"| Data | 0.5 | 1.5 |\n",
					"| **Total** | **1.02** | **3.0** |\n",
					"\n",
					"Not looking so good for the \"cloud-native\" way, huh? Stay tuned!\n",
					"\n",
					"## Lessons\n",
					"\n",
					"In the cloud-native approach, we only download data on demand. This works great for cloud-friendly file formats like Cloud Optimized GeoTIFF and Zarr.\n",
					"\n",
					"With `fsspec` reading *non*-cloud-optimzied file (HDF5 files), we can emulate the cloud-native workflow where we download data on demand. It's extremely convenient, but has *very* different performance characteristics compared to a local file system. In general, the cloud native approach works best when\n",
					"\n",
					"1. Metadata is in a consolidated location (true for COG, Zarr; not true for HDF5, grib)\n",
					"2. You're accessing a subset of the file (Think reading a small spatial subset of a large COG. A \"download\" model would download the whole file, wasting a bunch of bandwidth)\n",
					"3. You're accessing the data in parallel\n",
					"\n",
					"In the next couple notebooks we'll look at some better ways to access these data on the cloud. We'll see\n",
					"\n",
					"1. Kerchunk: Optimizes *metadata reads* from the *existing files*. A better cloud-optimzied layer on top of non-cloud-optimized files\n",
					"2. Convert the data to a cloud-friendly format (Zarr, geoparquet)"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Aside: Cloud-Native on the Planetary Computer\n",
					"\n",
					"It's perhaps worth seeing a bit what going all in on this cloud-native approach gets you.\n",
					"\n",
					"For example, the [Planetary Computer Explorer](https://planetarycomputer.microsoft.com/explore?c=124.0274%2C-16.4940&z=9.00&v=2) is a web application that lets you explore a bunch of datasets hosted on the Planetary Computer. The National Water Model isn't there yet, both because it's in NetCDF and isn't cataloged in STAC."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Next up, let's move to [using-kerchunk](using-kerchunk.ipynb)."
				]
			}
		]
	}
}